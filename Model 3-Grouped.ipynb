{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv')\n",
    "train_df = df[pd.to_datetime(df['quarter'])<=\"2019-01-01\"]\n",
    "test_df = df[pd.to_datetime(df['quarter'])==\"2019-04-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_df.copy()\n",
    "test_features=test_df.copy()\n",
    "train_mret=train_features[['mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4']]\n",
    "test_mret=test_features[['mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4']]\n",
    "train_labels = train_features.pop('weight')\n",
    "test_labels = test_features.pop('weight')\n",
    "train_features.drop(['quarter','mret_net_weighted_qt_L1','mret_net_weighted_qt_L2',\n",
    "                     'mret_net_weighted_qt_L3','mret_net_weighted_qt_L4'],axis=1,inplace=True)\n",
    "test_features.drop(['quarter','mret_net_weighted_qt_L1','mret_net_weighted_qt_L2',\n",
    "                    'mret_net_weighted_qt_L3','mret_net_weighted_qt_L4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nav_latest_L1', 'size_q_L1', 'yield_L1', 'per_com_L1',\n",
       "       'per_pref_L1', 'maturity_L1', 'exp_ratio_weighted_L1',\n",
       "       'turn_ratio_weighted_L1', 'age_L1', 'manager_tenure_L1', 'ioc_1',\n",
       "       'ioc_2', 'ioc_3', 'ioc_4', 'ioc_5', 'ioc_6', 'retail_fund_i',\n",
       "       'inst_fund_i', 'dead_flag_L1_i', 'lipper_asset_cd_1',\n",
       "       'lipper_asset_cd_2', 'lipper_asset_cd_3', 'prccq_L1', 'trt1q_L1',\n",
       "       'prccd_sd_L1', 'prccd_sd_qtr_L1', 'annret_L1', 'salegrowth_L1',\n",
       "       'size_L1', 'sizemkt_L1', 'bookleverage_L1', 'roa_L1',\n",
       "       'cashtoasset_L1', 'debtst_L1', 'debtlt_L1', 'q_L1', 'Pnaive_L1',\n",
       "       'buyback_L1', 'cshareissued_L1', 'netleverage_L1', 'capitalexp_L1',\n",
       "       'cashfvol_L1', 'trt1q_L2', 'trt1q_L3', 'trt1q_L4'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=np.array(train_features)\n",
    "test_features=np.array(test_features)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_mret=np.array(train_mret)\n",
    "test_mret=np.array(test_mret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data for Test and Training\n",
    "X_train = train_features\n",
    "Xexog_train = train_mret\n",
    "Y_train = train_labels\n",
    "\n",
    "X_test = test_features\n",
    "Xexog_test = test_mret\n",
    "Y_test = test_labels\n",
    "\n",
    "#Scale the predictors for training\n",
    "Xscaler_train =  MinMaxScaler(feature_range=(-1,1))\n",
    "X_scaled_train = Xscaler_train.fit_transform(X_train)\n",
    "\n",
    "Xexog_scaler_train =  MinMaxScaler(feature_range=(-1,1))\n",
    "Xexog_scaled_train = Xexog_scaler_train.fit_transform(Xexog_train)\n",
    "\n",
    "# Scale the data for testing using the in-sample transformation from earlier\n",
    "X_scaled_test = Xscaler_train.transform(X_test)\n",
    "Xexog_scaled_test = Xexog_scaler_train.transform(Xexog_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model fits a neural network for each group of macro variables and\n",
    "ensembles those macro networks. It also adds the yields variables in the\n",
    "last hidden layer.\n",
    "\"\"\"\n",
    "\n",
    "archi = [32, 16, 8]\n",
    "dropout_u = 0.3\n",
    "A = [6,10,7,8,13,13,6,6,10,10,1,1,1,1,1,1,1,1,1,1,1,1,2,5,2,2,2,9,5,5,3,7,2,11,3,2,2,2,4,3,9,9,11,12,2]\n",
    "\n",
    "# Keras requires 3D tuples for training.\n",
    "\n",
    "Xexog_scaled_train_expand = np.expand_dims(Xexog_scaled_train, axis=1)\n",
    "\n",
    "\n",
    "Y_train_expand1 = np.expand_dims(Y_train, axis=1)\n",
    "Y_train_expand = np.expand_dims(Y_train_expand1, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Xexog_scaled_test_expand = np.expand_dims(Xexog_scaled_test, axis=1)\n",
    "\n",
    "Y_test_expand1 = np.expand_dims(Y_test, axis=1)\n",
    "Y_test_expand = np.expand_dims(Y_test_expand1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X_train / X_test by group\n",
    "X_scaled_train_grouped = []\n",
    "X_scaled_test_grouped = []\n",
    "n_groups = len(np.unique(A))\n",
    "for i, group in enumerate(np.unique(A)):\n",
    "    temp = X_scaled_train[:,[x==group for x in A]]\n",
    "    X_scaled_train_grouped.append(np.expand_dims(temp, axis=1))\n",
    "    temp = X_scaled_test[:,[x==group for x in A]]\n",
    "    X_scaled_test_grouped.append(np.expand_dims(temp, axis=1))\n",
    "\n",
    "# seed numpy and tf\n",
    "tf.set_random_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "# Define Model Architecture\n",
    "n = len(archi)\n",
    "layers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 12)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 9)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1, 2)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1, 2)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1, 2)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 12)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 9)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 3)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 1)         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 3)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1, 3)         0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 2)         0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 1)         0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 3)         0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1, 3)         0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1, 2)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1, 1)         0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1, 2)         0           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 32)        416         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 32)        320         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 32)        128         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 32)        64          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 32)        128         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 32)        128         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 32)        96          dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1, 32)        64          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1, 32)        128         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1, 32)        128         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1, 32)        96          dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 32)        64          dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 32)        96          dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1, 32)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 32)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1, 32)        0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1, 32)        0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1, 32)        0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1, 32)        0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 1, 32)        0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1, 32)        0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 1, 32)        0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 32)        0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 1, 32)        0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 1, 32)        0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 1, 32)        0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 16)        528         dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 16)        528         dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 16)        528         dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 16)        528         dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 16)        528         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1, 16)        528         dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1, 16)        528         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1, 16)        528         dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 16)        528         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1, 16)        528         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1, 16)        528         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1, 16)        528         dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1, 16)        528         dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1, 16)        0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 1, 16)        0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1, 16)        0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 1, 16)        0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 1, 16)        0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 1, 16)        0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 1, 16)        0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 1, 16)        0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 1, 16)        0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 1, 16)        0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1, 16)        0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 1, 16)        0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 1, 16)        0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1, 8)         136         dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1, 8)         136         dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1, 8)         136         dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1, 8)         136         dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1, 8)         136         dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1, 8)         136         dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1, 8)         136         dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 1, 8)         136         dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1, 8)         136         dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1, 8)         136         dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1, 8)         136         dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1, 8)         136         dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1, 8)         136         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 104)       0           dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_30[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_34[0][0]                   \n",
      "                                                                 dense_35[0][0]                   \n",
      "                                                                 dense_36[0][0]                   \n",
      "                                                                 dense_37[0][0]                   \n",
      "                                                                 dense_38[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1, 4)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 108)       0           concatenate_1[0][0]              \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 1, 108)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 108)       432         dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1, 1)         109         batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 11,029\n",
      "Trainable params: 10,813\n",
      "Non-trainable params: 216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model for macro variables\n",
    "for i in range(n+1):\n",
    "    if i == 0:\n",
    "        layers['ins_main'] = [Input(shape=(1,X_scaled_train_grouped[j].shape[2])) for j in range(n_groups)]\n",
    "    elif i == 1:\n",
    "        layers['dropout'+str(i)] = [Dropout(dropout_u)(input_tensor) for input_tensor in layers['ins_main']]\n",
    "        layers['hiddens'+str(i)] = [Dense(archi[i-1], activation='relu',kernel_initializer='he_normal',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001))(input_tensor) for input_tensor in layers['dropout'+str(i)]]\n",
    "    elif i > 1 & i <= n:\n",
    "        layers['dropout'+str(i)] = [Dropout(dropout_u)(input_tensor) for input_tensor in layers['hiddens'+str(i-1)]]\n",
    "        layers['hiddens'+str(i)] = [Dense(archi[i-1], activation='relu',kernel_initializer='he_normal',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001))(input_tensor) for input_tensor in layers['dropout'+str(i)]]\n",
    "\n",
    "# Model for yiel variables\n",
    "layers['ins_exog'] = Input(shape=(1,Xexog_scaled_train_expand.shape[2]))\n",
    "# Merge macro group models\n",
    "layers['merge'] = concatenate(layers['hiddens'+str(n)])\n",
    "# Merge macro and yields models\n",
    "layers['merge1'] = concatenate([layers['merge'], layers['ins_exog']])\n",
    "layers['dropout_final'] = Dropout(dropout_u)(layers['merge1'])\n",
    "layers['BN'] = BatchNormalization()(layers['dropout_final'])\n",
    "layers['output'] = Dense(Y_train_expand.shape[2], kernel_initializer='he_normal')(layers['BN'])\n",
    "\n",
    "model = Model(inputs=layers['ins_main']+[layers['ins_exog']],\n",
    "              outputs=layers['output'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17203 samples, validate on 3036 samples\n",
      "Epoch 1/500\n",
      "17203/17203 [==============================] - 5s 282us/step - loss: 15.9799 - val_loss: 9.9620\n",
      "Epoch 2/500\n",
      "17203/17203 [==============================] - 3s 146us/step - loss: 8.3098 - val_loss: 6.9932\n",
      "Epoch 3/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 6.4191 - val_loss: 5.8176\n",
      "Epoch 4/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 5.5316 - val_loss: 5.1456\n",
      "Epoch 5/500\n",
      "17203/17203 [==============================] - 3s 155us/step - loss: 5.0247 - val_loss: 4.7794\n",
      "Epoch 6/500\n",
      "17203/17203 [==============================] - 3s 155us/step - loss: 4.6866 - val_loss: 4.4944\n",
      "Epoch 7/500\n",
      "17203/17203 [==============================] - 3s 154us/step - loss: 4.4582 - val_loss: 4.3042\n",
      "Epoch 8/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 4.2734 - val_loss: 4.1242\n",
      "Epoch 9/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 4.1274 - val_loss: 4.0077\n",
      "Epoch 10/500\n",
      "17203/17203 [==============================] - 3s 154us/step - loss: 4.0155 - val_loss: 3.8964\n",
      "Epoch 11/500\n",
      "17203/17203 [==============================] - 3s 154us/step - loss: 3.9109 - val_loss: 3.8085\n",
      "Epoch 12/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 3.8239 - val_loss: 3.7537\n",
      "Epoch 13/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 3.7465 - val_loss: 3.6536\n",
      "Epoch 14/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 3.6785 - val_loss: 3.5724\n",
      "Epoch 15/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 3.6230 - val_loss: 3.5335\n",
      "Epoch 16/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 3.5494 - val_loss: 3.4918\n",
      "Epoch 17/500\n",
      "17203/17203 [==============================] - 3s 155us/step - loss: 3.5006 - val_loss: 3.4177\n",
      "Epoch 18/500\n",
      "17203/17203 [==============================] - 3s 155us/step - loss: 3.4608 - val_loss: 3.3771\n",
      "Epoch 19/500\n",
      "17203/17203 [==============================] - 3s 154us/step - loss: 3.4112 - val_loss: 3.3481\n",
      "Epoch 20/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 3.3770 - val_loss: 3.3360\n",
      "Epoch 21/500\n",
      "17203/17203 [==============================] - 3s 155us/step - loss: 3.3410 - val_loss: 3.2727\n",
      "Epoch 22/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 3.3152 - val_loss: 3.2626\n",
      "Epoch 23/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 3.2832 - val_loss: 3.2051\n",
      "Epoch 24/500\n",
      "17203/17203 [==============================] - 3s 170us/step - loss: 3.2548 - val_loss: 3.2094\n",
      "Epoch 25/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 3.2295 - val_loss: 3.1678\n",
      "Epoch 26/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 3.1975 - val_loss: 3.1181\n",
      "Epoch 27/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 3.1780 - val_loss: 3.1328\n",
      "Epoch 28/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 3.1457 - val_loss: 3.1066\n",
      "Epoch 29/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 3.1325 - val_loss: 3.0720\n",
      "Epoch 30/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 3.1100 - val_loss: 3.0610\n",
      "Epoch 31/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 3.0964 - val_loss: 3.0460\n",
      "Epoch 32/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 3.0818 - val_loss: 3.0168\n",
      "Epoch 33/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 3.0606 - val_loss: 2.9927\n",
      "Epoch 34/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 3.0426 - val_loss: 2.9880\n",
      "Epoch 35/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 3.0341 - val_loss: 2.9596\n",
      "Epoch 36/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 3.0112 - val_loss: 2.9801\n",
      "Epoch 37/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.9980 - val_loss: 2.9543\n",
      "Epoch 38/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.9882 - val_loss: 2.9457\n",
      "Epoch 39/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.9742 - val_loss: 2.9200\n",
      "Epoch 40/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.9516 - val_loss: 2.9310\n",
      "Epoch 41/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.9460 - val_loss: 2.9061\n",
      "Epoch 42/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.9396 - val_loss: 2.8900\n",
      "Epoch 43/500\n",
      "17203/17203 [==============================] - 3s 167us/step - loss: 2.9252 - val_loss: 2.8963\n",
      "Epoch 44/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.9066 - val_loss: 2.8784\n",
      "Epoch 45/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.9050 - val_loss: 2.8495\n",
      "Epoch 46/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.8942 - val_loss: 2.8648\n",
      "Epoch 47/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.8876 - val_loss: 2.8533\n",
      "Epoch 48/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.8660 - val_loss: 2.8311\n",
      "Epoch 49/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.8586 - val_loss: 2.8106\n",
      "Epoch 50/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.8523 - val_loss: 2.8251\n",
      "Epoch 51/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.8400 - val_loss: 2.8018\n",
      "Epoch 52/500\n",
      "17203/17203 [==============================] - 3s 172us/step - loss: 2.8384 - val_loss: 2.7985\n",
      "Epoch 53/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.8208 - val_loss: 2.7997\n",
      "Epoch 54/500\n",
      "17203/17203 [==============================] - 3s 167us/step - loss: 2.8305 - val_loss: 2.7916\n",
      "Epoch 55/500\n",
      "17203/17203 [==============================] - 3s 174us/step - loss: 2.8073 - val_loss: 2.7886\n",
      "Epoch 56/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.7969 - val_loss: 2.7922\n",
      "Epoch 57/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7932 - val_loss: 2.7682\n",
      "Epoch 58/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.7774 - val_loss: 2.7394\n",
      "Epoch 59/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.7756 - val_loss: 2.7639\n",
      "Epoch 60/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7794 - val_loss: 2.7420\n",
      "Epoch 61/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.7684 - val_loss: 2.7495\n",
      "Epoch 62/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7520 - val_loss: 2.7437\n",
      "Epoch 63/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.7481 - val_loss: 2.7350\n",
      "Epoch 64/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7426 - val_loss: 2.7206\n",
      "Epoch 65/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.7415 - val_loss: 2.7183\n",
      "Epoch 66/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.7261 - val_loss: 2.7000\n",
      "Epoch 67/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 2.7300 - val_loss: 2.7106\n",
      "Epoch 68/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.7205 - val_loss: 2.6857\n",
      "Epoch 69/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7226 - val_loss: 2.6966\n",
      "Epoch 70/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.7079 - val_loss: 2.6908\n",
      "Epoch 71/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.7039 - val_loss: 2.6818\n",
      "Epoch 72/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.6920 - val_loss: 2.6837\n",
      "Epoch 73/500\n",
      "17203/17203 [==============================] - 3s 169us/step - loss: 2.6912 - val_loss: 2.6841\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.6863 - val_loss: 2.6784\n",
      "Epoch 75/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.6857 - val_loss: 2.6663\n",
      "Epoch 76/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.6698 - val_loss: 2.6648\n",
      "Epoch 77/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.6687 - val_loss: 2.6531\n",
      "Epoch 78/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.6685 - val_loss: 2.6491\n",
      "Epoch 79/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 2.6562 - val_loss: 2.6396\n",
      "Epoch 80/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.6574 - val_loss: 2.6343\n",
      "Epoch 81/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.6471 - val_loss: 2.6462\n",
      "Epoch 82/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.6555 - val_loss: 2.6303\n",
      "Epoch 83/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.6473 - val_loss: 2.6265\n",
      "Epoch 84/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.6446 - val_loss: 2.6116\n",
      "Epoch 85/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 2.6386 - val_loss: 2.6195\n",
      "Epoch 86/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.6322 - val_loss: 2.6175\n",
      "Epoch 87/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.6194 - val_loss: 2.6059\n",
      "Epoch 88/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.6145 - val_loss: 2.5883\n",
      "Epoch 89/500\n",
      "17203/17203 [==============================] - 3s 179us/step - loss: 2.6108 - val_loss: 2.6045\n",
      "Epoch 90/500\n",
      "17203/17203 [==============================] - 3s 171us/step - loss: 2.6204 - val_loss: 2.5952\n",
      "Epoch 91/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.6120 - val_loss: 2.5956\n",
      "Epoch 92/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.6114 - val_loss: 2.5860\n",
      "Epoch 93/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5944 - val_loss: 2.5896\n",
      "Epoch 94/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.6012 - val_loss: 2.5844\n",
      "Epoch 95/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.5959 - val_loss: 2.5790\n",
      "Epoch 96/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5966 - val_loss: 2.5766\n",
      "Epoch 97/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5964 - val_loss: 2.5762\n",
      "Epoch 98/500\n",
      "17203/17203 [==============================] - 3s 170us/step - loss: 2.5798 - val_loss: 2.5676\n",
      "Epoch 99/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.5754 - val_loss: 2.5687\n",
      "Epoch 100/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5767 - val_loss: 2.5565\n",
      "Epoch 101/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.5756 - val_loss: 2.5571\n",
      "Epoch 102/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5633 - val_loss: 2.5559\n",
      "Epoch 103/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5739 - val_loss: 2.5543\n",
      "Epoch 104/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.5591 - val_loss: 2.5493\n",
      "Epoch 105/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.5612 - val_loss: 2.5447\n",
      "Epoch 106/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.5580 - val_loss: 2.5503\n",
      "Epoch 107/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5520 - val_loss: 2.5434\n",
      "Epoch 108/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5464 - val_loss: 2.5369\n",
      "Epoch 109/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5442 - val_loss: 2.5283\n",
      "Epoch 110/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.5379 - val_loss: 2.5312\n",
      "Epoch 111/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.5460 - val_loss: 2.5230\n",
      "Epoch 112/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5448 - val_loss: 2.5219\n",
      "Epoch 113/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5371 - val_loss: 2.5179\n",
      "Epoch 114/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.5216 - val_loss: 2.5207\n",
      "Epoch 115/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5317 - val_loss: 2.5120\n",
      "Epoch 116/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5270 - val_loss: 2.5136\n",
      "Epoch 117/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 2.5234 - val_loss: 2.5084\n",
      "Epoch 118/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.5234 - val_loss: 2.5057\n",
      "Epoch 119/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.5219 - val_loss: 2.5104\n",
      "Epoch 120/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.5042 - val_loss: 2.5106\n",
      "Epoch 121/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.5095 - val_loss: 2.5088\n",
      "Epoch 122/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4944 - val_loss: 2.5023\n",
      "Epoch 123/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.5116 - val_loss: 2.4889\n",
      "Epoch 124/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.5057 - val_loss: 2.4899\n",
      "Epoch 125/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4921 - val_loss: 2.4826\n",
      "Epoch 126/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.4996 - val_loss: 2.4946\n",
      "Epoch 127/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4887 - val_loss: 2.4891\n",
      "Epoch 128/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4975 - val_loss: 2.4781\n",
      "Epoch 129/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.4893 - val_loss: 2.4799\n",
      "Epoch 130/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4794 - val_loss: 2.4791\n",
      "Epoch 131/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4817 - val_loss: 2.4753\n",
      "Epoch 132/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4808 - val_loss: 2.4686\n",
      "Epoch 133/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4750 - val_loss: 2.4712\n",
      "Epoch 134/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.4839 - val_loss: 2.4701\n",
      "Epoch 135/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4688 - val_loss: 2.4624\n",
      "Epoch 136/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.4740 - val_loss: 2.4617\n",
      "Epoch 137/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4672 - val_loss: 2.4596\n",
      "Epoch 138/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4820 - val_loss: 2.4515\n",
      "Epoch 139/500\n",
      "17203/17203 [==============================] - 3s 171us/step - loss: 2.4643 - val_loss: 2.4615\n",
      "Epoch 140/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4621 - val_loss: 2.4564\n",
      "Epoch 141/500\n",
      "17203/17203 [==============================] - 3s 167us/step - loss: 2.4629 - val_loss: 2.4491\n",
      "Epoch 142/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.4553 - val_loss: 2.4555\n",
      "Epoch 143/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4604 - val_loss: 2.4450\n",
      "Epoch 144/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4495 - val_loss: 2.4400\n",
      "Epoch 145/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4531 - val_loss: 2.4385\n",
      "Epoch 146/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4460 - val_loss: 2.4344\n",
      "Epoch 147/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.4545 - val_loss: 2.4352\n",
      "Epoch 148/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4569 - val_loss: 2.4309\n",
      "Epoch 149/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4454 - val_loss: 2.4371\n",
      "Epoch 150/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4318 - val_loss: 2.4332\n",
      "Epoch 151/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.4446 - val_loss: 2.4294\n",
      "Epoch 152/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.4458 - val_loss: 2.4306\n",
      "Epoch 153/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4388 - val_loss: 2.4313\n",
      "Epoch 154/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4295 - val_loss: 2.4276\n",
      "Epoch 155/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4341 - val_loss: 2.4185\n",
      "Epoch 156/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.4246 - val_loss: 2.4185\n",
      "Epoch 157/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4326 - val_loss: 2.4174\n",
      "Epoch 158/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4319 - val_loss: 2.4132\n",
      "Epoch 159/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4251 - val_loss: 2.4160\n",
      "Epoch 160/500\n",
      "17203/17203 [==============================] - 3s 167us/step - loss: 2.4325 - val_loss: 2.4110\n",
      "Epoch 161/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.4273 - val_loss: 2.4078\n",
      "Epoch 162/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4187 - val_loss: 2.4076\n",
      "Epoch 163/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.4094 - val_loss: 2.4078\n",
      "Epoch 164/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4172 - val_loss: 2.4062\n",
      "Epoch 165/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4110 - val_loss: 2.4019\n",
      "Epoch 166/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4086 - val_loss: 2.4030\n",
      "Epoch 167/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.4141 - val_loss: 2.4015\n",
      "Epoch 168/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4096 - val_loss: 2.3964\n",
      "Epoch 169/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4093 - val_loss: 2.3927\n",
      "Epoch 170/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4074 - val_loss: 2.3929\n",
      "Epoch 171/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4055 - val_loss: 2.3962\n",
      "Epoch 172/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3988 - val_loss: 2.4005\n",
      "Epoch 173/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.4016 - val_loss: 2.3873\n",
      "Epoch 174/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4057 - val_loss: 2.3883\n",
      "Epoch 175/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3920 - val_loss: 2.3828\n",
      "Epoch 176/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.4000 - val_loss: 2.3901\n",
      "Epoch 177/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3898 - val_loss: 2.3823\n",
      "Epoch 178/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.4012 - val_loss: 2.3798\n",
      "Epoch 179/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3964 - val_loss: 2.3730\n",
      "Epoch 180/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3892 - val_loss: 2.3709\n",
      "Epoch 181/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3903 - val_loss: 2.3758\n",
      "Epoch 182/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3979 - val_loss: 2.3833\n",
      "Epoch 183/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3861 - val_loss: 2.3711\n",
      "Epoch 184/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3812 - val_loss: 2.3771\n",
      "Epoch 185/500\n",
      "17203/17203 [==============================] - 3s 182us/step - loss: 2.3780 - val_loss: 2.3688\n",
      "Epoch 186/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3744 - val_loss: 2.3777\n",
      "Epoch 187/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3701 - val_loss: 2.3737\n",
      "Epoch 188/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3887 - val_loss: 2.3634\n",
      "Epoch 189/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3783 - val_loss: 2.3641\n",
      "Epoch 190/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3702 - val_loss: 2.3631\n",
      "Epoch 191/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3658 - val_loss: 2.3677\n",
      "Epoch 192/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3713 - val_loss: 2.3610\n",
      "Epoch 193/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3639 - val_loss: 2.3597\n",
      "Epoch 194/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3785 - val_loss: 2.3544\n",
      "Epoch 195/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3548 - val_loss: 2.3554\n",
      "Epoch 196/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3641 - val_loss: 2.3566\n",
      "Epoch 197/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3672 - val_loss: 2.3528\n",
      "Epoch 198/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3596 - val_loss: 2.3494\n",
      "Epoch 199/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 2.3571 - val_loss: 2.3494\n",
      "Epoch 200/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3649 - val_loss: 2.3455\n",
      "Epoch 201/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3604 - val_loss: 2.3506\n",
      "Epoch 202/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3547 - val_loss: 2.3489\n",
      "Epoch 203/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3531 - val_loss: 2.3450\n",
      "Epoch 204/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 2.3461 - val_loss: 2.3426\n",
      "Epoch 205/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3586 - val_loss: 2.3408\n",
      "Epoch 206/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3457 - val_loss: 2.3434\n",
      "Epoch 207/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3543 - val_loss: 2.3384\n",
      "Epoch 208/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3487 - val_loss: 2.3348\n",
      "Epoch 209/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3454 - val_loss: 2.3352\n",
      "Epoch 210/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3380 - val_loss: 2.3375\n",
      "Epoch 211/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3465 - val_loss: 2.3298\n",
      "Epoch 212/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3412 - val_loss: 2.3293\n",
      "Epoch 213/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3459 - val_loss: 2.3279\n",
      "Epoch 214/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3493 - val_loss: 2.3263\n",
      "Epoch 215/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3411 - val_loss: 2.3267\n",
      "Epoch 216/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3354 - val_loss: 2.3228\n",
      "Epoch 217/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3366 - val_loss: 2.3239\n",
      "Epoch 218/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3383 - val_loss: 2.3241\n",
      "Epoch 219/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3313 - val_loss: 2.3212\n",
      "Epoch 220/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3337 - val_loss: 2.3184\n",
      "Epoch 221/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3401 - val_loss: 2.3149\n",
      "Epoch 222/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3352 - val_loss: 2.3179\n",
      "Epoch 223/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3264 - val_loss: 2.3178\n",
      "Epoch 224/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3287 - val_loss: 2.3178\n",
      "Epoch 225/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3358 - val_loss: 2.3093\n",
      "Epoch 226/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.3193 - val_loss: 2.3169\n",
      "Epoch 227/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3264 - val_loss: 2.3165\n",
      "Epoch 228/500\n",
      "17203/17203 [==============================] - 3s 171us/step - loss: 2.3215 - val_loss: 2.3122\n",
      "Epoch 229/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3247 - val_loss: 2.3136\n",
      "Epoch 230/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3247 - val_loss: 2.3074\n",
      "Epoch 231/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3164 - val_loss: 2.3089\n",
      "Epoch 232/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.3204 - val_loss: 2.3038\n",
      "Epoch 233/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.3132 - val_loss: 2.3037\n",
      "Epoch 234/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3156 - val_loss: 2.3071\n",
      "Epoch 235/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.3111 - val_loss: 2.3058\n",
      "Epoch 236/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3035 - val_loss: 2.3053\n",
      "Epoch 237/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3084 - val_loss: 2.3057\n",
      "Epoch 238/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.3219 - val_loss: 2.2991\n",
      "Epoch 239/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3119 - val_loss: 2.3059\n",
      "Epoch 240/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3133 - val_loss: 2.3034\n",
      "Epoch 241/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.3061 - val_loss: 2.3014\n",
      "Epoch 242/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3083 - val_loss: 2.2952\n",
      "Epoch 243/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2966 - val_loss: 2.2973\n",
      "Epoch 244/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3067 - val_loss: 2.2918\n",
      "Epoch 245/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3119 - val_loss: 2.2908\n",
      "Epoch 246/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3004 - val_loss: 2.2981\n",
      "Epoch 247/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2961 - val_loss: 2.2948\n",
      "Epoch 248/500\n",
      "17203/17203 [==============================] - 3s 172us/step - loss: 2.3023 - val_loss: 2.2907\n",
      "Epoch 249/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.3016 - val_loss: 2.2882\n",
      "Epoch 250/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2988 - val_loss: 2.2898\n",
      "Epoch 251/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2919 - val_loss: 2.2824\n",
      "Epoch 252/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.3039 - val_loss: 2.2906\n",
      "Epoch 253/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3037 - val_loss: 2.2824\n",
      "Epoch 254/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2963 - val_loss: 2.2851\n",
      "Epoch 255/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.3008 - val_loss: 2.2848\n",
      "Epoch 256/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2923 - val_loss: 2.2819\n",
      "Epoch 257/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2984 - val_loss: 2.2760\n",
      "Epoch 258/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2838 - val_loss: 2.2847\n",
      "Epoch 259/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.3021 - val_loss: 2.2782\n",
      "Epoch 260/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2859 - val_loss: 2.2840\n",
      "Epoch 261/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.3013 - val_loss: 2.2712\n",
      "Epoch 262/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2938 - val_loss: 2.2713\n",
      "Epoch 263/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2863 - val_loss: 2.2749\n",
      "Epoch 264/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2903 - val_loss: 2.2754\n",
      "Epoch 265/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2820 - val_loss: 2.2747\n",
      "Epoch 266/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.2850 - val_loss: 2.2674\n",
      "Epoch 267/500\n",
      "17203/17203 [==============================] - 3s 174us/step - loss: 2.2742 - val_loss: 2.2715\n",
      "Epoch 268/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2749 - val_loss: 2.2698\n",
      "Epoch 269/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2869 - val_loss: 2.2646\n",
      "Epoch 270/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2792 - val_loss: 2.2641\n",
      "Epoch 271/500\n",
      "17203/17203 [==============================] - 3s 172us/step - loss: 2.2833 - val_loss: 2.2641\n",
      "Epoch 272/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.2823 - val_loss: 2.2670\n",
      "Epoch 273/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2776 - val_loss: 2.2624\n",
      "Epoch 274/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2752 - val_loss: 2.2664\n",
      "Epoch 275/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2771 - val_loss: 2.2630\n",
      "Epoch 276/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2791 - val_loss: 2.2597\n",
      "Epoch 277/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2763 - val_loss: 2.2598\n",
      "Epoch 278/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2787 - val_loss: 2.2542\n",
      "Epoch 279/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2606 - val_loss: 2.2653\n",
      "Epoch 280/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2717 - val_loss: 2.2594\n",
      "Epoch 281/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2664 - val_loss: 2.2563\n",
      "Epoch 282/500\n",
      "17203/17203 [==============================] - 3s 169us/step - loss: 2.2706 - val_loss: 2.2578\n",
      "Epoch 283/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2731 - val_loss: 2.2571\n",
      "Epoch 284/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.2619 - val_loss: 2.2533\n",
      "Epoch 285/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2730 - val_loss: 2.2552\n",
      "Epoch 286/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2667 - val_loss: 2.2543\n",
      "Epoch 287/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.2657 - val_loss: 2.2542\n",
      "Epoch 288/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2539 - val_loss: 2.2525\n",
      "Epoch 289/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2698 - val_loss: 2.2520\n",
      "Epoch 290/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2597 - val_loss: 2.2506\n",
      "Epoch 291/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 2.2565 - val_loss: 2.2498\n",
      "Epoch 292/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2675 - val_loss: 2.2510\n",
      "Epoch 293/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2544 - val_loss: 2.2461\n",
      "Epoch 294/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2708 - val_loss: 2.2407\n",
      "Epoch 295/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2660 - val_loss: 2.2470\n",
      "Epoch 296/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2548 - val_loss: 2.2425\n",
      "Epoch 297/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2496 - val_loss: 2.2463\n",
      "Epoch 298/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2562 - val_loss: 2.2419\n",
      "Epoch 299/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2519 - val_loss: 2.2453\n",
      "Epoch 300/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.2682 - val_loss: 2.2387\n",
      "Epoch 301/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2501 - val_loss: 2.2387\n",
      "Epoch 302/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2554 - val_loss: 2.2376\n",
      "Epoch 303/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2493 - val_loss: 2.2459\n",
      "Epoch 304/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2489 - val_loss: 2.2425\n",
      "Epoch 305/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2522 - val_loss: 2.2374\n",
      "Epoch 306/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2500 - val_loss: 2.2379\n",
      "Epoch 307/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2417 - val_loss: 2.2346\n",
      "Epoch 308/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2526 - val_loss: 2.2326\n",
      "Epoch 309/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2480 - val_loss: 2.2389\n",
      "Epoch 310/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2413 - val_loss: 2.2455\n",
      "Epoch 311/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2404 - val_loss: 2.2383\n",
      "Epoch 312/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2474 - val_loss: 2.2286\n",
      "Epoch 313/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.2422 - val_loss: 2.2332\n",
      "Epoch 314/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2462 - val_loss: 2.2321\n",
      "Epoch 315/500\n",
      "17203/17203 [==============================] - 3s 179us/step - loss: 2.2466 - val_loss: 2.2301\n",
      "Epoch 316/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2467 - val_loss: 2.2299\n",
      "Epoch 317/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2472 - val_loss: 2.2228\n",
      "Epoch 318/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2489 - val_loss: 2.2278\n",
      "Epoch 319/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2276 - val_loss: 2.2323\n",
      "Epoch 320/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2441 - val_loss: 2.2259\n",
      "Epoch 321/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2328 - val_loss: 2.2279\n",
      "Epoch 322/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2462 - val_loss: 2.2309\n",
      "Epoch 323/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.2264 - val_loss: 2.2271\n",
      "Epoch 324/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.2306 - val_loss: 2.2284\n",
      "Epoch 325/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2366 - val_loss: 2.2199\n",
      "Epoch 326/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2353 - val_loss: 2.2234\n",
      "Epoch 327/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2275 - val_loss: 2.2222\n",
      "Epoch 328/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.2392 - val_loss: 2.2209\n",
      "Epoch 329/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2433 - val_loss: 2.2150\n",
      "Epoch 330/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2289 - val_loss: 2.2187\n",
      "Epoch 331/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2333 - val_loss: 2.2169\n",
      "Epoch 332/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2306 - val_loss: 2.2189\n",
      "Epoch 333/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2365 - val_loss: 2.2221\n",
      "Epoch 334/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2341 - val_loss: 2.2188\n",
      "Epoch 335/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.2339 - val_loss: 2.2172\n",
      "Epoch 336/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2335 - val_loss: 2.2239\n",
      "Epoch 337/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2283 - val_loss: 2.2102\n",
      "Epoch 338/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2204 - val_loss: 2.2169\n",
      "Epoch 339/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2307 - val_loss: 2.2115\n",
      "Epoch 340/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.2351 - val_loss: 2.2122\n",
      "Epoch 341/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2294 - val_loss: 2.2108\n",
      "Epoch 342/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2332 - val_loss: 2.2102\n",
      "Epoch 343/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2334 - val_loss: 2.2140\n",
      "Epoch 344/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2155 - val_loss: 2.2130\n",
      "Epoch 345/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2245 - val_loss: 2.2085\n",
      "Epoch 346/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2142 - val_loss: 2.2085\n",
      "Epoch 347/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2272 - val_loss: 2.2089\n",
      "Epoch 348/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2257 - val_loss: 2.2048\n",
      "Epoch 349/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2206 - val_loss: 2.2077\n",
      "Epoch 350/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2177 - val_loss: 2.2036\n",
      "Epoch 351/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2312 - val_loss: 2.2016\n",
      "Epoch 352/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2208 - val_loss: 2.2045\n",
      "Epoch 353/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2168 - val_loss: 2.2065\n",
      "Epoch 354/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2133 - val_loss: 2.2065\n",
      "Epoch 355/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2121 - val_loss: 2.2073\n",
      "Epoch 356/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2121 - val_loss: 2.2053\n",
      "Epoch 357/500\n",
      "17203/17203 [==============================] - 3s 168us/step - loss: 2.2090 - val_loss: 2.2015\n",
      "Epoch 358/500\n",
      "17203/17203 [==============================] - 3s 173us/step - loss: 2.2198 - val_loss: 2.2033\n",
      "Epoch 359/500\n",
      "17203/17203 [==============================] - 3s 167us/step - loss: 2.2160 - val_loss: 2.2020\n",
      "Epoch 360/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2127 - val_loss: 2.1992\n",
      "Epoch 361/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2116 - val_loss: 2.2016\n",
      "Epoch 362/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2143 - val_loss: 2.2053\n",
      "Epoch 363/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2098 - val_loss: 2.2064\n",
      "Epoch 364/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2125 - val_loss: 2.1975\n",
      "Epoch 365/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2161 - val_loss: 2.2046\n",
      "Epoch 366/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2126 - val_loss: 2.2003\n",
      "Epoch 367/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2090 - val_loss: 2.1934\n",
      "Epoch 368/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2058 - val_loss: 2.1967\n",
      "Epoch 369/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.2064 - val_loss: 2.1955\n",
      "Epoch 370/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.2048 - val_loss: 2.1952\n",
      "Epoch 371/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.2025 - val_loss: 2.1952\n",
      "Epoch 372/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2021 - val_loss: 2.1940\n",
      "Epoch 373/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1999 - val_loss: 2.2010\n",
      "Epoch 374/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.1986 - val_loss: 2.1953\n",
      "Epoch 375/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1997 - val_loss: 2.1941\n",
      "Epoch 376/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2111 - val_loss: 2.1904\n",
      "Epoch 377/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.2038 - val_loss: 2.1955\n",
      "Epoch 378/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.2003 - val_loss: 2.1882\n",
      "Epoch 379/500\n",
      "17203/17203 [==============================] - 3s 165us/step - loss: 2.2066 - val_loss: 2.1909\n",
      "Epoch 380/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.2063 - val_loss: 2.1926\n",
      "Epoch 381/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1968 - val_loss: 2.1901\n",
      "Epoch 382/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1995 - val_loss: 2.1862\n",
      "Epoch 383/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1950 - val_loss: 2.1859\n",
      "Epoch 384/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2003 - val_loss: 2.1909\n",
      "Epoch 385/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1989 - val_loss: 2.1918\n",
      "Epoch 386/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1962 - val_loss: 2.1918\n",
      "Epoch 387/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1939 - val_loss: 2.1874\n",
      "Epoch 388/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.1962 - val_loss: 2.1864\n",
      "Epoch 389/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1925 - val_loss: 2.1858\n",
      "Epoch 390/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1922 - val_loss: 2.1837\n",
      "Epoch 391/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1896 - val_loss: 2.1840\n",
      "Epoch 392/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1821 - val_loss: 2.1883\n",
      "Epoch 393/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1936 - val_loss: 2.1840\n",
      "Epoch 394/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1991 - val_loss: 2.1816\n",
      "Epoch 395/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1903 - val_loss: 2.1777\n",
      "Epoch 396/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1957 - val_loss: 2.1815\n",
      "Epoch 397/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1899 - val_loss: 2.1796\n",
      "Epoch 398/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1882 - val_loss: 2.1809\n",
      "Epoch 399/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1820 - val_loss: 2.1813\n",
      "Epoch 400/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1836 - val_loss: 2.1826\n",
      "Epoch 401/500\n",
      "17203/17203 [==============================] - 3s 169us/step - loss: 2.1992 - val_loss: 2.1748\n",
      "Epoch 402/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.1935 - val_loss: 2.1761\n",
      "Epoch 403/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1870 - val_loss: 2.1790\n",
      "Epoch 404/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1877 - val_loss: 2.1776\n",
      "Epoch 405/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1829 - val_loss: 2.1798\n",
      "Epoch 406/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1779 - val_loss: 2.1768\n",
      "Epoch 407/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.2001 - val_loss: 2.1845\n",
      "Epoch 408/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.1819 - val_loss: 2.1738\n",
      "Epoch 409/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1930 - val_loss: 2.1733\n",
      "Epoch 410/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1838 - val_loss: 2.1752\n",
      "Epoch 411/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1886 - val_loss: 2.1741\n",
      "Epoch 412/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1812 - val_loss: 2.1750\n",
      "Epoch 413/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1909 - val_loss: 2.1749\n",
      "Epoch 414/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1815 - val_loss: 2.1697\n",
      "Epoch 415/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.1788 - val_loss: 2.1720\n",
      "Epoch 416/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1760 - val_loss: 2.1740\n",
      "Epoch 417/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.1846 - val_loss: 2.1692\n",
      "Epoch 418/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1812 - val_loss: 2.1711\n",
      "Epoch 419/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1907 - val_loss: 2.1687\n",
      "Epoch 420/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.1775 - val_loss: 2.1735\n",
      "Epoch 421/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1756 - val_loss: 2.1736\n",
      "Epoch 422/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.1833 - val_loss: 2.1629\n",
      "Epoch 423/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.1728 - val_loss: 2.1661\n",
      "Epoch 424/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1712 - val_loss: 2.1682\n",
      "Epoch 425/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1884 - val_loss: 2.1624\n",
      "Epoch 426/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1756 - val_loss: 2.1746\n",
      "Epoch 427/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1656 - val_loss: 2.1656\n",
      "Epoch 428/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1737 - val_loss: 2.1626\n",
      "Epoch 429/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1788 - val_loss: 2.1689\n",
      "Epoch 430/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1714 - val_loss: 2.1731\n",
      "Epoch 431/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1825 - val_loss: 2.1635\n",
      "Epoch 432/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1782 - val_loss: 2.1655\n",
      "Epoch 433/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1671 - val_loss: 2.1609\n",
      "Epoch 434/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1730 - val_loss: 2.1586\n",
      "Epoch 435/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1643 - val_loss: 2.1603\n",
      "Epoch 436/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1781 - val_loss: 2.1642\n",
      "Epoch 437/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1766 - val_loss: 2.1586\n",
      "Epoch 438/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1688 - val_loss: 2.1601\n",
      "Epoch 439/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1706 - val_loss: 2.1587\n",
      "Epoch 440/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1695 - val_loss: 2.1554\n",
      "Epoch 441/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1671 - val_loss: 2.1599\n",
      "Epoch 442/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1675 - val_loss: 2.1622\n",
      "Epoch 443/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1631 - val_loss: 2.1614\n",
      "Epoch 444/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.1682 - val_loss: 2.1586\n",
      "Epoch 445/500\n",
      "17203/17203 [==============================] - 3s 174us/step - loss: 2.1690 - val_loss: 2.1577\n",
      "Epoch 446/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1686 - val_loss: 2.1550\n",
      "Epoch 447/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1792 - val_loss: 2.1495\n",
      "Epoch 448/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1717 - val_loss: 2.1586\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.1649 - val_loss: 2.1591\n",
      "Epoch 450/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1717 - val_loss: 2.1536\n",
      "Epoch 451/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1702 - val_loss: 2.1523\n",
      "Epoch 452/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1635 - val_loss: 2.1583\n",
      "Epoch 453/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1650 - val_loss: 2.1503\n",
      "Epoch 454/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1703 - val_loss: 2.1512\n",
      "Epoch 455/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.1687 - val_loss: 2.1512\n",
      "Epoch 456/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1708 - val_loss: 2.1553\n",
      "Epoch 457/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1709 - val_loss: 2.1498\n",
      "Epoch 458/500\n",
      "17203/17203 [==============================] - 3s 171us/step - loss: 2.1617 - val_loss: 2.1542\n",
      "Epoch 459/500\n",
      "17203/17203 [==============================] - 3s 156us/step - loss: 2.1603 - val_loss: 2.1513\n",
      "Epoch 460/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1584 - val_loss: 2.1515\n",
      "Epoch 461/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1650 - val_loss: 2.1504\n",
      "Epoch 462/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1740 - val_loss: 2.1548\n",
      "Epoch 463/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1616 - val_loss: 2.1533\n",
      "Epoch 464/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1571 - val_loss: 2.1490\n",
      "Epoch 465/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1671 - val_loss: 2.1481\n",
      "Epoch 466/500\n",
      "17203/17203 [==============================] - 3s 166us/step - loss: 2.1550 - val_loss: 2.1478\n",
      "Epoch 467/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1550 - val_loss: 2.1518\n",
      "Epoch 468/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1652 - val_loss: 2.1454\n",
      "Epoch 469/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1507 - val_loss: 2.1525\n",
      "Epoch 470/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1656 - val_loss: 2.1425\n",
      "Epoch 471/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1619 - val_loss: 2.1430\n",
      "Epoch 472/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.1611 - val_loss: 2.1423\n",
      "Epoch 473/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1634 - val_loss: 2.1454\n",
      "Epoch 474/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1543 - val_loss: 2.1431\n",
      "Epoch 475/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1626 - val_loss: 2.1428\n",
      "Epoch 476/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1579 - val_loss: 2.1423\n",
      "Epoch 477/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.1557 - val_loss: 2.1396\n",
      "Epoch 478/500\n",
      "17203/17203 [==============================] - 3s 157us/step - loss: 2.1487 - val_loss: 2.1494\n",
      "Epoch 479/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1429 - val_loss: 2.1442\n",
      "Epoch 480/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1562 - val_loss: 2.1416\n",
      "Epoch 481/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1528 - val_loss: 2.1463\n",
      "Epoch 482/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1498 - val_loss: 2.1399\n",
      "Epoch 483/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1586 - val_loss: 2.1411\n",
      "Epoch 484/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1595 - val_loss: 2.1420\n",
      "Epoch 485/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1539 - val_loss: 2.1346\n",
      "Epoch 486/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1506 - val_loss: 2.1418\n",
      "Epoch 487/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1538 - val_loss: 2.1360\n",
      "Epoch 488/500\n",
      "17203/17203 [==============================] - 3s 175us/step - loss: 2.1508 - val_loss: 2.1387\n",
      "Epoch 489/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1442 - val_loss: 2.1354\n",
      "Epoch 490/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1518 - val_loss: 2.1395\n",
      "Epoch 491/500\n",
      "17203/17203 [==============================] - 3s 159us/step - loss: 2.1502 - val_loss: 2.1377\n",
      "Epoch 492/500\n",
      "17203/17203 [==============================] - 3s 163us/step - loss: 2.1535 - val_loss: 2.1335\n",
      "Epoch 493/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1546 - val_loss: 2.1391\n",
      "Epoch 494/500\n",
      "17203/17203 [==============================] - 3s 164us/step - loss: 2.1507 - val_loss: 2.1340\n",
      "Epoch 495/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1493 - val_loss: 2.1356\n",
      "Epoch 496/500\n",
      "17203/17203 [==============================] - 3s 161us/step - loss: 2.1473 - val_loss: 2.1356\n",
      "Epoch 497/500\n",
      "17203/17203 [==============================] - 3s 160us/step - loss: 2.1433 - val_loss: 2.1368\n",
      "Epoch 498/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1450 - val_loss: 2.1362\n",
      "Epoch 499/500\n",
      "17203/17203 [==============================] - 3s 158us/step - loss: 2.1447 - val_loss: 2.1345\n",
      "Epoch 500/500\n",
      "17203/17203 [==============================] - 3s 162us/step - loss: 2.1543 - val_loss: 2.1282\n",
      "2017/2017 [==============================] - 0s 45us/step\n",
      "Test score: 2.0165495655001187\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "sgd_fine = SGD(lr=0.015, momentum=0.9, decay=0.01, nesterov=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=1e-6,\n",
    "                              patience=20, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd_fine)\n",
    "history=model.fit(X_scaled_train_grouped+[Xexog_scaled_train_expand], Y_train_expand,\n",
    "                  epochs=500,validation_split=0.15, batch_size=32, shuffle=True,verbose=1)\n",
    "\n",
    "# Perform Forecast and Test Model\n",
    "Ypred = model.predict(X_scaled_test_grouped+[Xexog_scaled_test_expand])\n",
    "Ypred = np.squeeze(Ypred,axis=1)\n",
    "\n",
    "score = model.evaluate(X_scaled_test_grouped+[Xexog_scaled_test_expand], Y_test_expand, verbose=1)\n",
    "print(\"Test score:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
