{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm \n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/99722/Desktop/ML/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[pd.to_datetime(df['quarter'])<=\"2019-01-01\"]\n",
    "test_df = df[pd.to_datetime(df['quarter'])==\"2019-04-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=train_df.copy()\n",
    "test_features=test_df.copy()\n",
    "train_mret=train_features[['mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4']]\n",
    "test_mret=test_features[['mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4']]\n",
    "train_dum=train_features[['ioc_1','ioc_2','ioc_3','ioc_4','ioc_5','ioc_6','retail_fund_i','inst_fund_i',\n",
    "                    'dead_flag_L1_i','lipper_asset_cd_1','lipper_asset_cd_2','lipper_asset_cd_3']]\n",
    "test_dum=test_features[['ioc_1','ioc_2','ioc_3','ioc_4','ioc_5','ioc_6','retail_fund_i','inst_fund_i',\n",
    "                    'dead_flag_L1_i','lipper_asset_cd_1','lipper_asset_cd_2','lipper_asset_cd_3']]\n",
    "train_labels = train_features.pop('weight')\n",
    "test_labels = test_features.pop('weight')\n",
    "train_features.drop(['ioc_1','ioc_2','ioc_3','ioc_4','ioc_5','ioc_6','retail_fund_i','inst_fund_i',\n",
    "                    'dead_flag_L1_i','lipper_asset_cd_1','lipper_asset_cd_2','lipper_asset_cd_3','quarter',\n",
    "                     'mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4'],axis=1,inplace=True)\n",
    "test_features.drop(['ioc_1','ioc_2','ioc_3','ioc_4','ioc_5','ioc_6','retail_fund_i','inst_fund_i',\n",
    "                    'dead_flag_L1_i','lipper_asset_cd_1','lipper_asset_cd_2','lipper_asset_cd_3','quarter',\n",
    "                   'mret_net_weighted_qt_L1','mret_net_weighted_qt_L2','mret_net_weighted_qt_L3','mret_net_weighted_qt_L4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=np.array(train_features)\n",
    "test_features=np.array(test_features)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_mret=np.array(train_mret)\n",
    "test_mret=np.array(test_mret)\n",
    "train_dum=np.array(train_dum)\n",
    "test_dum=np.array(test_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data for Test and Training\n",
    "X_train = train_features\n",
    "Xexog_train = train_mret\n",
    "Xdum_train = train_dum\n",
    "Y_train = train_labels\n",
    "\n",
    "X_test = test_features\n",
    "Xexog_test = test_mret\n",
    "Xdum_test = test_dum\n",
    "Y_test = test_labels\n",
    "\n",
    "#Scale the predictors for training\n",
    "Xscaler_train =  MinMaxScaler(feature_range=(-1,1))\n",
    "X_scaled_train = Xscaler_train.fit_transform(X_train)\n",
    "\n",
    "Xexog_scaler_train =  MinMaxScaler(feature_range=(-1,1))\n",
    "Xexog_scaled_train = Xexog_scaler_train.fit_transform(Xexog_train)\n",
    "\n",
    "# Scale the data for testing using the in-sample transformation from earlier\n",
    "X_scaled_test = Xscaler_train.transform(X_test)\n",
    "Xexog_scaled_test = Xexog_scaler_train.transform(Xexog_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[0.17520728 0.2765309  0.37106169 0.44411807 0.508157   0.56744595\n",
      " 0.61318463 0.65580052 0.69681704 0.73117879 0.76210615 0.79137655\n",
      " 0.81657032 0.83760208 0.85719204 0.87629784 0.89144233 0.90580182\n",
      " 0.91857496 0.93034575 0.94119833 0.95086684 0.96006118 0.96846541\n",
      " 0.97672623 0.98439383 0.99028619 0.99507793 0.99843655 0.99921984\n",
      " 0.99986655 0.99995779 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2583fe67d88>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Get the number, d,  of PCA needed to explain 95% of the variance\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled_train)\n",
    "sumv = np.cumsum(pca.explained_variance_ratio_) # a vector of cumulative sums of the elements  \n",
    "\n",
    "d = np.argmax(sumv >= 0.7) + 1\n",
    "\n",
    "print(d)\n",
    "\n",
    "print(sumv)\n",
    "plt.plot(sumv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = PCA(n_components = 10)\n",
    "F_train=pca_train.fit_transform(X_scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[0.20031062 0.32941859 0.43986941 0.51786201 0.58325868 0.64360807\n",
      " 0.68952603 0.73235822 0.77040615 0.79693798 0.82257632 0.84143103\n",
      " 0.8600496  0.87652494 0.89254949 0.90668597 0.92033106 0.93145209\n",
      " 0.94136074 0.95073633 0.95971264 0.96835656 0.97529476 0.98142573\n",
      " 0.98698427 0.99157195 0.99501492 0.99729696 0.99853511 0.99927323\n",
      " 0.99991304 0.99996369 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2583feb5708>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled_test)\n",
    "sumv = np.cumsum(pca.explained_variance_ratio_) # a vector of cumulative sums of the elements  \n",
    "\n",
    "d = np.argmax(sumv >= 0.7) + 1\n",
    "\n",
    "print(d)\n",
    "\n",
    "print(sumv)\n",
    "plt.plot(sumv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA(n_components = 10)\n",
    "F_test=pca_test.fit_transform(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 10)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 32)        352         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 32)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 16)        528         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 16)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 8)         136         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 4)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 12)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 24)        0           dense_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 24)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 24)        96          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1)         25          batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,137\n",
      "Trainable params: 1,089\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "archi = [32, 16, 8]\n",
    "dropout_u = 0.3\n",
    "\n",
    "\n",
    "\n",
    "# Keras requires 3D tuples for training.\n",
    "F_train_expand = np.expand_dims(F_train, axis=1)\n",
    "Xexog_scaled_train_expand = np.expand_dims(Xexog_scaled_train, axis=1)\n",
    "Xdum_train_expand = np.expand_dims(Xdum_train, axis=1)\n",
    "\n",
    "Y_train_expand1 = np.expand_dims(Y_train, axis=1)\n",
    "Y_train_expand2 = np.expand_dims(Y_train_expand1, axis=1)\n",
    "\n",
    "Y_test_expand1 = np.expand_dims(Y_test, axis=1)\n",
    "Y_test_expand2 = np.expand_dims(Y_test_expand1, axis=1)\n",
    "\n",
    "# seed numpy and tf\n",
    "tf.set_random_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "# Define Model Architecture\n",
    "\n",
    "# Base model for macro variables\n",
    "n = len(archi)\n",
    "layers = dict()\n",
    "for i in range(n+1):\n",
    "    if i == 0:\n",
    "        layers['ins_main'] = Input(shape=(1,F_train_expand.shape[2]))\n",
    "    elif i == 1:\n",
    "        layers['dropout'+str(i)] = Dropout(dropout_u)(layers['ins_main'])\n",
    "        layers['hidden'+str(i)] = Dense(archi[i-1],kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001),bias_initializer='he_normal', \n",
    "                                        kernel_initializer='he_normal', activation='relu')(layers['dropout'+str(i)])\n",
    "    elif i > 1 & i <= n:\n",
    "        layers['dropout'+str(i)] = Dropout(dropout_u)(layers['hidden'+str(i-1)])\n",
    "        layers['hidden'+str(i)] = Dense(archi[i-1],kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001),bias_initializer='he_normal', \n",
    "                                        kernel_initializer='he_normal', activation='relu')(layers['dropout'+str(i)])\n",
    "\n",
    "# Model for yield variables\n",
    "layers['ins_exog'] = Input(shape=(1,Xexog_scaled_train_expand.shape[2]))\n",
    "\n",
    "# Model for dummy variables\n",
    "layers['ins_dum'] = Input(shape=(1,Xdum_train_expand.shape[2]))\n",
    "\n",
    "# Merge macro / yield networks\n",
    "layers['merge'] = concatenate([layers['hidden'+str(n)], layers['ins_exog'],layers['ins_dum'] ])\n",
    "layers['dropout_final'] = Dropout(dropout_u)(layers['merge'])\n",
    "layers['BN'] = BatchNormalization()(layers['dropout_final'])\n",
    "layers['output'] = Dense(Y_train_expand2.shape[2],bias_initializer='he_normal',\n",
    "                         kernel_initializer='he_normal')(layers['BN'])\n",
    "\n",
    "model = Model(inputs=[layers['ins_main'], layers['ins_exog'],layers['ins_dum']], outputs=layers['output'])\n",
    "\n",
    "# Compile model\n",
    "sgd_fine = SGD(lr=0.01, momentum=0.9, decay=0.01, nesterov=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss',min_delta=1e-6,\n",
    "                              patience=20,verbose=0, mode='auto')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17203 samples, validate on 3036 samples\n",
      "Epoch 1/300\n",
      "17203/17203 [==============================] - 1s 57us/step - loss: 3.5290 - val_loss: 2.9000\n",
      "Epoch 2/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 2.8019 - val_loss: 2.5895\n",
      "Epoch 3/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 2.5775 - val_loss: 2.4109\n",
      "Epoch 4/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 2.4572 - val_loss: 2.3231\n",
      "Epoch 5/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 2.3717 - val_loss: 2.2651\n",
      "Epoch 6/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 2.3137 - val_loss: 2.1903\n",
      "Epoch 7/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 2.2646 - val_loss: 2.1639\n",
      "Epoch 8/300\n",
      "17203/17203 [==============================] - 1s 42us/step - loss: 2.2308 - val_loss: 2.1440\n",
      "Epoch 9/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 2.2005 - val_loss: 2.0961\n",
      "Epoch 10/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 2.1728 - val_loss: 2.0845\n",
      "Epoch 11/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 2.1501 - val_loss: 2.0525\n",
      "Epoch 12/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 2.1341 - val_loss: 2.0497\n",
      "Epoch 13/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 2.1164 - val_loss: 2.0207\n",
      "Epoch 14/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 2.0997 - val_loss: 2.0083\n",
      "Epoch 15/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 2.0847 - val_loss: 1.9944\n",
      "Epoch 16/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 2.0790 - val_loss: 1.9863\n",
      "Epoch 17/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 2.0608 - val_loss: 1.9584\n",
      "Epoch 18/300\n",
      "17203/17203 [==============================] - 1s 51us/step - loss: 2.0555 - val_loss: 1.9739\n",
      "Epoch 19/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 2.0431 - val_loss: 1.9603\n",
      "Epoch 20/300\n",
      "17203/17203 [==============================] - 1s 54us/step - loss: 2.0374 - val_loss: 1.9467\n",
      "Epoch 21/300\n",
      "17203/17203 [==============================] - 1s 51us/step - loss: 2.0271 - val_loss: 1.9446\n",
      "Epoch 22/300\n",
      "17203/17203 [==============================] - 1s 66us/step - loss: 2.0218 - val_loss: 1.9289\n",
      "Epoch 23/300\n",
      "17203/17203 [==============================] - 1s 86us/step - loss: 2.0141 - val_loss: 1.9186\n",
      "Epoch 24/300\n",
      "17203/17203 [==============================] - 2s 90us/step - loss: 2.0080 - val_loss: 1.9456\n",
      "Epoch 25/300\n",
      "17203/17203 [==============================] - 1s 81us/step - loss: 2.0065 - val_loss: 1.9310\n",
      "Epoch 26/300\n",
      "17203/17203 [==============================] - 1s 47us/step - loss: 1.9997 - val_loss: 1.9165\n",
      "Epoch 27/300\n",
      "17203/17203 [==============================] - 1s 50us/step - loss: 1.9957 - val_loss: 1.8983\n",
      "Epoch 28/300\n",
      "17203/17203 [==============================] - 1s 55us/step - loss: 1.9889 - val_loss: 1.9019\n",
      "Epoch 29/300\n",
      "17203/17203 [==============================] - 1s 63us/step - loss: 1.9863 - val_loss: 1.9106\n",
      "Epoch 30/300\n",
      "17203/17203 [==============================] - 1s 53us/step - loss: 1.9816 - val_loss: 1.8895\n",
      "Epoch 31/300\n",
      "17203/17203 [==============================] - 1s 49us/step - loss: 1.9742 - val_loss: 1.8996\n",
      "Epoch 32/300\n",
      "17203/17203 [==============================] - 1s 57us/step - loss: 1.9722 - val_loss: 1.8874\n",
      "Epoch 33/300\n",
      "17203/17203 [==============================] - 1s 51us/step - loss: 1.9649 - val_loss: 1.8877\n",
      "Epoch 34/300\n",
      "17203/17203 [==============================] - 1s 52us/step - loss: 1.9652 - val_loss: 1.9017\n",
      "Epoch 35/300\n",
      "17203/17203 [==============================] - 1s 50us/step - loss: 1.9627 - val_loss: 1.8790\n",
      "Epoch 36/300\n",
      "17203/17203 [==============================] - 1s 53us/step - loss: 1.9627 - val_loss: 1.8767\n",
      "Epoch 37/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.9562 - val_loss: 1.8645\n",
      "Epoch 38/300\n",
      "17203/17203 [==============================] - 1s 42us/step - loss: 1.9489 - val_loss: 1.8703\n",
      "Epoch 39/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9524 - val_loss: 1.8742\n",
      "Epoch 40/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9481 - val_loss: 1.8572\n",
      "Epoch 41/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.9452 - val_loss: 1.8741\n",
      "Epoch 42/300\n",
      "17203/17203 [==============================] - 1s 42us/step - loss: 1.9454 - val_loss: 1.8595\n",
      "Epoch 43/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9384 - val_loss: 1.8526\n",
      "Epoch 44/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.9448 - val_loss: 1.8538\n",
      "Epoch 45/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9373 - val_loss: 1.8560\n",
      "Epoch 46/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.9343 - val_loss: 1.8477\n",
      "Epoch 47/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9405 - val_loss: 1.8468\n",
      "Epoch 48/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9296 - val_loss: 1.8412\n",
      "Epoch 49/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.9298 - val_loss: 1.8563\n",
      "Epoch 50/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.9279 - val_loss: 1.8346\n",
      "Epoch 51/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.9223 - val_loss: 1.8430\n",
      "Epoch 52/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.9257 - val_loss: 1.8375\n",
      "Epoch 53/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.9253 - val_loss: 1.8322\n",
      "Epoch 54/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.9244 - val_loss: 1.8296\n",
      "Epoch 55/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.9201 - val_loss: 1.8440\n",
      "Epoch 56/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.9180 - val_loss: 1.8241\n",
      "Epoch 57/300\n",
      "17203/17203 [==============================] - 1s 52us/step - loss: 1.9181 - val_loss: 1.8304\n",
      "Epoch 58/300\n",
      "17203/17203 [==============================] - 1s 58us/step - loss: 1.9146 - val_loss: 1.8344\n",
      "Epoch 59/300\n",
      "17203/17203 [==============================] - 1s 59us/step - loss: 1.9170 - val_loss: 1.8519\n",
      "Epoch 60/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.9125 - val_loss: 1.8209\n",
      "Epoch 61/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9116 - val_loss: 1.8251\n",
      "Epoch 62/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.9161 - val_loss: 1.8593\n",
      "Epoch 63/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9096 - val_loss: 1.8341\n",
      "Epoch 64/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9080 - val_loss: 1.8155\n",
      "Epoch 65/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9063 - val_loss: 1.8203\n",
      "Epoch 66/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.9053 - val_loss: 1.8236\n",
      "Epoch 67/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.9036 - val_loss: 1.8233\n",
      "Epoch 68/300\n",
      "17203/17203 [==============================] - 1s 47us/step - loss: 1.9021 - val_loss: 1.8197\n",
      "Epoch 69/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.8981 - val_loss: 1.8084\n",
      "Epoch 70/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.9008 - val_loss: 1.8203\n",
      "Epoch 71/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8963 - val_loss: 1.8121\n",
      "Epoch 72/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8979 - val_loss: 1.8337\n",
      "Epoch 73/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.8966 - val_loss: 1.8279\n",
      "Epoch 74/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.8947 - val_loss: 1.8045\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8940 - val_loss: 1.8049\n",
      "Epoch 76/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.8958 - val_loss: 1.8168\n",
      "Epoch 77/300\n",
      "17203/17203 [==============================] - 1s 50us/step - loss: 1.8949 - val_loss: 1.8016\n",
      "Epoch 78/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.8955 - val_loss: 1.8104\n",
      "Epoch 79/300\n",
      "17203/17203 [==============================] - 1s 47us/step - loss: 1.8881 - val_loss: 1.8006\n",
      "Epoch 80/300\n",
      "17203/17203 [==============================] - 1s 46us/step - loss: 1.8931 - val_loss: 1.8008\n",
      "Epoch 81/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.8896 - val_loss: 1.8008\n",
      "Epoch 82/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8930 - val_loss: 1.7969\n",
      "Epoch 83/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8919 - val_loss: 1.8067\n",
      "Epoch 84/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8919 - val_loss: 1.8024\n",
      "Epoch 85/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8853 - val_loss: 1.7922\n",
      "Epoch 86/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8859 - val_loss: 1.7938\n",
      "Epoch 87/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8878 - val_loss: 1.7948\n",
      "Epoch 88/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8863 - val_loss: 1.8118\n",
      "Epoch 89/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8811 - val_loss: 1.7879\n",
      "Epoch 90/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8881 - val_loss: 1.7982\n",
      "Epoch 91/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8782 - val_loss: 1.8064\n",
      "Epoch 92/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8830 - val_loss: 1.7983\n",
      "Epoch 93/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8773 - val_loss: 1.8036\n",
      "Epoch 94/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8826 - val_loss: 1.7940\n",
      "Epoch 95/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8758 - val_loss: 1.7866\n",
      "Epoch 96/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8778 - val_loss: 1.7887\n",
      "Epoch 97/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8766 - val_loss: 1.7958\n",
      "Epoch 98/300\n",
      "17203/17203 [==============================] - 1s 34us/step - loss: 1.8790 - val_loss: 1.7896\n",
      "Epoch 99/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8782 - val_loss: 1.7916\n",
      "Epoch 100/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8754 - val_loss: 1.7882\n",
      "Epoch 101/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8773 - val_loss: 1.7901\n",
      "Epoch 102/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8724 - val_loss: 1.7831\n",
      "Epoch 103/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8733 - val_loss: 1.7983\n",
      "Epoch 104/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8701 - val_loss: 1.7917\n",
      "Epoch 105/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8745 - val_loss: 1.7845\n",
      "Epoch 106/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8727 - val_loss: 1.7938\n",
      "Epoch 107/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8693 - val_loss: 1.7792\n",
      "Epoch 108/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8708 - val_loss: 1.7828\n",
      "Epoch 109/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8740 - val_loss: 1.7813\n",
      "Epoch 110/300\n",
      "17203/17203 [==============================] - 1s 34us/step - loss: 1.8722 - val_loss: 1.7900\n",
      "Epoch 111/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8665 - val_loss: 1.7873\n",
      "Epoch 112/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8683 - val_loss: 1.7718\n",
      "Epoch 113/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8723 - val_loss: 1.7776\n",
      "Epoch 114/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8681 - val_loss: 1.7836\n",
      "Epoch 115/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8694 - val_loss: 1.7845\n",
      "Epoch 116/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8678 - val_loss: 1.7896\n",
      "Epoch 117/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8673 - val_loss: 1.7912\n",
      "Epoch 118/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8642 - val_loss: 1.7727\n",
      "Epoch 119/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8657 - val_loss: 1.7770\n",
      "Epoch 120/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8677 - val_loss: 1.7749\n",
      "Epoch 121/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8645 - val_loss: 1.8028\n",
      "Epoch 122/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8612 - val_loss: 1.7799\n",
      "Epoch 123/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8633 - val_loss: 1.7851\n",
      "Epoch 124/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8683 - val_loss: 1.7991\n",
      "Epoch 125/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8653 - val_loss: 1.7785\n",
      "Epoch 126/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8599 - val_loss: 1.7819\n",
      "Epoch 127/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8624 - val_loss: 1.7851\n",
      "Epoch 128/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8638 - val_loss: 1.7787\n",
      "Epoch 129/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8633 - val_loss: 1.7768\n",
      "Epoch 130/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8627 - val_loss: 1.7740\n",
      "Epoch 131/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8636 - val_loss: 1.7753\n",
      "Epoch 132/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8599 - val_loss: 1.7694\n",
      "Epoch 133/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8583 - val_loss: 1.7897\n",
      "Epoch 134/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8579 - val_loss: 1.7777\n",
      "Epoch 135/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8596 - val_loss: 1.7846\n",
      "Epoch 136/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8586 - val_loss: 1.7777\n",
      "Epoch 137/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8612 - val_loss: 1.7681\n",
      "Epoch 138/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8559 - val_loss: 1.7696\n",
      "Epoch 139/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8556 - val_loss: 1.7709\n",
      "Epoch 140/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8613 - val_loss: 1.7939\n",
      "Epoch 141/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8544 - val_loss: 1.7670\n",
      "Epoch 142/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8506 - val_loss: 1.7750\n",
      "Epoch 143/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8573 - val_loss: 1.7641\n",
      "Epoch 144/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8542 - val_loss: 1.7804\n",
      "Epoch 145/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8572 - val_loss: 1.7807\n",
      "Epoch 146/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8567 - val_loss: 1.7693\n",
      "Epoch 147/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8565 - val_loss: 1.7656\n",
      "Epoch 148/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8544 - val_loss: 1.7615\n",
      "Epoch 149/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8525 - val_loss: 1.7725\n",
      "Epoch 150/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8544 - val_loss: 1.7661\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203/17203 [==============================] - 1s 34us/step - loss: 1.8560 - val_loss: 1.7654\n",
      "Epoch 152/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8531 - val_loss: 1.7762\n",
      "Epoch 153/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8522 - val_loss: 1.7627\n",
      "Epoch 154/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8531 - val_loss: 1.7619\n",
      "Epoch 155/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8500 - val_loss: 1.7606\n",
      "Epoch 156/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8535 - val_loss: 1.7660\n",
      "Epoch 157/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8484 - val_loss: 1.7651\n",
      "Epoch 158/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8516 - val_loss: 1.7647\n",
      "Epoch 159/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8495 - val_loss: 1.7719\n",
      "Epoch 160/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8486 - val_loss: 1.7629\n",
      "Epoch 161/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8492 - val_loss: 1.7688\n",
      "Epoch 162/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8507 - val_loss: 1.7591\n",
      "Epoch 163/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8536 - val_loss: 1.7859\n",
      "Epoch 164/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8480 - val_loss: 1.7606\n",
      "Epoch 165/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8498 - val_loss: 1.7716\n",
      "Epoch 166/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8524 - val_loss: 1.7612\n",
      "Epoch 167/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8521 - val_loss: 1.7612\n",
      "Epoch 168/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8497 - val_loss: 1.7582\n",
      "Epoch 169/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8457 - val_loss: 1.7637\n",
      "Epoch 170/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8503 - val_loss: 1.7650\n",
      "Epoch 171/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8488 - val_loss: 1.7644\n",
      "Epoch 172/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8483 - val_loss: 1.7704\n",
      "Epoch 173/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8464 - val_loss: 1.7612\n",
      "Epoch 174/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8451 - val_loss: 1.7527\n",
      "Epoch 175/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8485 - val_loss: 1.7652\n",
      "Epoch 176/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8454 - val_loss: 1.7719\n",
      "Epoch 177/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8455 - val_loss: 1.7540\n",
      "Epoch 178/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8512 - val_loss: 1.7609\n",
      "Epoch 179/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8479 - val_loss: 1.7590\n",
      "Epoch 180/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8442 - val_loss: 1.7592\n",
      "Epoch 181/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8454 - val_loss: 1.7714\n",
      "Epoch 182/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8434 - val_loss: 1.7553\n",
      "Epoch 183/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8461 - val_loss: 1.7507\n",
      "Epoch 184/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8441 - val_loss: 1.7648\n",
      "Epoch 185/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8412 - val_loss: 1.7546\n",
      "Epoch 186/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8484 - val_loss: 1.7608\n",
      "Epoch 187/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8442 - val_loss: 1.7484\n",
      "Epoch 188/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8423 - val_loss: 1.7569\n",
      "Epoch 189/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8465 - val_loss: 1.7633\n",
      "Epoch 190/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8459 - val_loss: 1.7502\n",
      "Epoch 191/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8437 - val_loss: 1.7575\n",
      "Epoch 192/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8404 - val_loss: 1.7577\n",
      "Epoch 193/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8393 - val_loss: 1.7575\n",
      "Epoch 194/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8413 - val_loss: 1.7620\n",
      "Epoch 195/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8429 - val_loss: 1.7538\n",
      "Epoch 196/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8442 - val_loss: 1.7736\n",
      "Epoch 197/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8419 - val_loss: 1.7600\n",
      "Epoch 198/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8438 - val_loss: 1.7670\n",
      "Epoch 199/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8447 - val_loss: 1.7574\n",
      "Epoch 200/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8435 - val_loss: 1.7726\n",
      "Epoch 201/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8449 - val_loss: 1.7533\n",
      "Epoch 202/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8379 - val_loss: 1.7562\n",
      "Epoch 203/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8427 - val_loss: 1.7563\n",
      "Epoch 204/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8362 - val_loss: 1.7523\n",
      "Epoch 205/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8417 - val_loss: 1.7433\n",
      "Epoch 206/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8432 - val_loss: 1.7462\n",
      "Epoch 207/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8432 - val_loss: 1.7577\n",
      "Epoch 208/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8444 - val_loss: 1.7614\n",
      "Epoch 209/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8381 - val_loss: 1.7593\n",
      "Epoch 210/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8400 - val_loss: 1.7477\n",
      "Epoch 211/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8387 - val_loss: 1.7564\n",
      "Epoch 212/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8415 - val_loss: 1.7496\n",
      "Epoch 213/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8420 - val_loss: 1.7527\n",
      "Epoch 214/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8396 - val_loss: 1.7591\n",
      "Epoch 215/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8398 - val_loss: 1.7673\n",
      "Epoch 216/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8378 - val_loss: 1.7487\n",
      "Epoch 217/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8347 - val_loss: 1.7734\n",
      "Epoch 218/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8394 - val_loss: 1.7451\n",
      "Epoch 219/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8408 - val_loss: 1.7484\n",
      "Epoch 220/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8428 - val_loss: 1.7569\n",
      "Epoch 221/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8390 - val_loss: 1.7421\n",
      "Epoch 222/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8366 - val_loss: 1.7582\n",
      "Epoch 223/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8412 - val_loss: 1.7590\n",
      "Epoch 224/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8362 - val_loss: 1.7560\n",
      "Epoch 225/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8373 - val_loss: 1.7514\n",
      "Epoch 226/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8384 - val_loss: 1.7519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8337 - val_loss: 1.7517\n",
      "Epoch 228/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8399 - val_loss: 1.7541\n",
      "Epoch 229/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8374 - val_loss: 1.7460\n",
      "Epoch 230/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8349 - val_loss: 1.7462\n",
      "Epoch 231/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8386 - val_loss: 1.7453\n",
      "Epoch 232/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8347 - val_loss: 1.7519\n",
      "Epoch 233/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8318 - val_loss: 1.7582\n",
      "Epoch 234/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8377 - val_loss: 1.7517\n",
      "Epoch 235/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8346 - val_loss: 1.7486\n",
      "Epoch 236/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8344 - val_loss: 1.7394\n",
      "Epoch 237/300\n",
      "17203/17203 [==============================] - 1s 56us/step - loss: 1.8343 - val_loss: 1.7489\n",
      "Epoch 238/300\n",
      "17203/17203 [==============================] - 1s 60us/step - loss: 1.8345 - val_loss: 1.7650\n",
      "Epoch 239/300\n",
      "17203/17203 [==============================] - 1s 57us/step - loss: 1.8396 - val_loss: 1.7674\n",
      "Epoch 240/300\n",
      "17203/17203 [==============================] - 1s 42us/step - loss: 1.8335 - val_loss: 1.7467\n",
      "Epoch 241/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8391 - val_loss: 1.7551\n",
      "Epoch 242/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8352 - val_loss: 1.7581\n",
      "Epoch 243/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8359 - val_loss: 1.7432\n",
      "Epoch 244/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8366 - val_loss: 1.7564\n",
      "Epoch 245/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.8343 - val_loss: 1.7462\n",
      "Epoch 246/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8360 - val_loss: 1.7463\n",
      "Epoch 247/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8324 - val_loss: 1.7570\n",
      "Epoch 248/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8322 - val_loss: 1.7527\n",
      "Epoch 249/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8357 - val_loss: 1.7441\n",
      "Epoch 250/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8301 - val_loss: 1.7516\n",
      "Epoch 251/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8303 - val_loss: 1.7586\n",
      "Epoch 252/300\n",
      "17203/17203 [==============================] - 1s 45us/step - loss: 1.8354 - val_loss: 1.7421\n",
      "Epoch 253/300\n",
      "17203/17203 [==============================] - 1s 44us/step - loss: 1.8360 - val_loss: 1.7487\n",
      "Epoch 254/300\n",
      "17203/17203 [==============================] - 1s 42us/step - loss: 1.8326 - val_loss: 1.7393\n",
      "Epoch 255/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8316 - val_loss: 1.7553\n",
      "Epoch 256/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8362 - val_loss: 1.7491\n",
      "Epoch 257/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8322 - val_loss: 1.7521\n",
      "Epoch 258/300\n",
      "17203/17203 [==============================] - 1s 43us/step - loss: 1.8297 - val_loss: 1.7414\n",
      "Epoch 259/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8362 - val_loss: 1.7493\n",
      "Epoch 260/300\n",
      "17203/17203 [==============================] - 1s 41us/step - loss: 1.8336 - val_loss: 1.7575\n",
      "Epoch 261/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8330 - val_loss: 1.7486\n",
      "Epoch 262/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8350 - val_loss: 1.7615\n",
      "Epoch 263/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8336 - val_loss: 1.7446\n",
      "Epoch 264/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8328 - val_loss: 1.7487\n",
      "Epoch 265/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8332 - val_loss: 1.7552\n",
      "Epoch 266/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8299 - val_loss: 1.7552\n",
      "Epoch 267/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8263 - val_loss: 1.7480\n",
      "Epoch 268/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8356 - val_loss: 1.7765\n",
      "Epoch 269/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8309 - val_loss: 1.7451\n",
      "Epoch 270/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8330 - val_loss: 1.7449\n",
      "Epoch 271/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8290 - val_loss: 1.7389\n",
      "Epoch 272/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8321 - val_loss: 1.7518\n",
      "Epoch 273/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8265 - val_loss: 1.7360\n",
      "Epoch 274/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8313 - val_loss: 1.7569\n",
      "Epoch 275/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8274 - val_loss: 1.7511\n",
      "Epoch 276/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8292 - val_loss: 1.7418\n",
      "Epoch 277/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8307 - val_loss: 1.7373\n",
      "Epoch 278/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8308 - val_loss: 1.7515\n",
      "Epoch 279/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8302 - val_loss: 1.7409\n",
      "Epoch 280/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8338 - val_loss: 1.7483\n",
      "Epoch 281/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8281 - val_loss: 1.7476\n",
      "Epoch 282/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8333 - val_loss: 1.7484\n",
      "Epoch 283/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8310 - val_loss: 1.7411\n",
      "Epoch 284/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8270 - val_loss: 1.7449\n",
      "Epoch 285/300\n",
      "17203/17203 [==============================] - 1s 39us/step - loss: 1.8298 - val_loss: 1.7534\n",
      "Epoch 286/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8304 - val_loss: 1.7388\n",
      "Epoch 287/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8279 - val_loss: 1.7431\n",
      "Epoch 288/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8307 - val_loss: 1.7379\n",
      "Epoch 289/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8295 - val_loss: 1.7409\n",
      "Epoch 290/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8269 - val_loss: 1.7405\n",
      "Epoch 291/300\n",
      "17203/17203 [==============================] - 1s 37us/step - loss: 1.8272 - val_loss: 1.7459\n",
      "Epoch 292/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8279 - val_loss: 1.7505\n",
      "Epoch 293/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8283 - val_loss: 1.7440\n",
      "Epoch 294/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8326 - val_loss: 1.7454\n",
      "Epoch 295/300\n",
      "17203/17203 [==============================] - 1s 35us/step - loss: 1.8265 - val_loss: 1.7418\n",
      "Epoch 296/300\n",
      "17203/17203 [==============================] - 1s 36us/step - loss: 1.8275 - val_loss: 1.7502\n",
      "Epoch 297/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8304 - val_loss: 1.7511\n",
      "Epoch 298/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8308 - val_loss: 1.7523\n",
      "Epoch 299/300\n",
      "17203/17203 [==============================] - 1s 38us/step - loss: 1.8303 - val_loss: 1.7463\n",
      "Epoch 300/300\n",
      "17203/17203 [==============================] - 1s 40us/step - loss: 1.8249 - val_loss: 1.7384\n",
      "2017/2017 [==============================] - 0s 12us/step\n",
      "Test score: 1.63205787056321\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=sgd_fine)\n",
    "history=model.fit([F_train_expand, Xexog_scaled_train_expand, Xdum_train_expand], Y_train_expand2, epochs=300,\n",
    "                  validation_split=0.15, batch_size=32, shuffle=True, verbose=1)\n",
    "\n",
    "# Scale the data for testing using the in-sample transformation from earlier\n",
    "F_test_expand = np.expand_dims(F_test, axis=1)\n",
    "Xexog_scaled_test_expand = np.expand_dims(Xexog_scaled_test, axis=1)\n",
    "Xdum_test_expand = np.expand_dims(Xdum_test, axis=1)\n",
    "\n",
    "\n",
    "# Make out-of-sample prediction on the unseen observations\n",
    "Ypred = model.predict([F_test_expand,Xexog_scaled_test_expand,Xdum_test_expand])\n",
    "Ypred = np.squeeze(Ypred,axis=1)\n",
    "\n",
    "score = model.evaluate([F_test_expand,Xexog_scaled_test_expand,Xdum_test_expand], Y_test_expand2, verbose=1)\n",
    "print(\"Test score:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
